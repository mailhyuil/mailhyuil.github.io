"use strict";(globalThis.webpackChunkmy_blog=globalThis.webpackChunkmy_blog||[]).push([[12323],{303451:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"ai langchain/langchain LangGraph custom agent","title":"langchain LangGraph custom agent","description":"","source":"@site/docs/ai langchain/langchain LangGraph custom agent.md","sourceDirName":"ai langchain","slug":"/ai langchain/langchain LangGraph custom agent","permalink":"/docs/ai langchain/langchain LangGraph custom agent","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"langchain LangGraph createReactAgent","permalink":"/docs/ai langchain/langchain LangGraph createReactAgent"},"next":{"title":"langchain LangGraph graph (workflow)","permalink":"/docs/ai langchain/langchain LangGraph graph - workflow"}}');var o=t(447259),s=t(529087);const r={},i="langchain LangGraph custom agent",l={},c=[];function g(n){const e={code:"code",h1:"h1",header:"header",pre:"pre",...(0,s.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"langchain-langgraph-custom-agent",children:"langchain LangGraph custom agent"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-ts",children:'import { TavilySearchResults } from "@langchain/community/tools/tavily_search";\r\nimport { ChatOpenAI } from "@langchain/openai";\r\nimport { HumanMessage, AIMessage } from "@langchain/core/messages";\r\nimport { ToolNode } from "@langchain/langgraph/prebuilt";\r\nimport { StateGraph, MessagesAnnotation } from "@langchain/langgraph";\r\n\r\n// Define the tools for the agent to use\r\nconst tools = [new TavilySearchResults({ maxResults: 3 })];\r\nconst toolNode = new ToolNode(tools);\r\n\r\n// Create a model and give it access to the tools\r\nconst model = new ChatOpenAI({\r\n  model: "gpt-4o-mini",\r\n  temperature: 0,\r\n}).bindTools(tools);\r\n\r\n// Define the function that determines whether to continue or not\r\n// \uacc4\uc18d\ud560\uc9c0 \ub9d0\uc9c0 \uacb0\uc815\ud558\ub294 \ud568\uc218 \uc815\uc758\r\nfunction shouldContinue({ messages }: typeof MessagesAnnotation.State) {\r\n  const lastMessage = messages[messages.length - 1] as AIMessage;\r\n\r\n  // If the LLM makes a tool call, then we route to the "tools" node\r\n  // \ub3c4\uad6c \ud638\ucd9c\uc774 \uc788\uc73c\uba74 "tools" \ub178\ub4dc\ub85c \ub77c\uc6b0\ud305\r\n  if (lastMessage.tool_calls?.length) {\r\n    return "tools";\r\n  }\r\n  // Otherwise, we stop (reply to the user) using the special "__end__" node\r\n  // \uadf8\ub807\uc9c0 \uc54a\uc73c\uba74 \ud2b9\uc218\ud55c "__end__" \ub178\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec \uba48\ucda4\r\n  return "__end__";\r\n}\r\n\r\n// Define the function that calls the model\r\n// \ubaa8\ub378\uc744 \ud638\ucd9c\ud558\ub294 \ud568\uc218 \uc815\uc758\r\nasync function callModel(state: typeof MessagesAnnotation.State) {\r\n  const response = await model.invoke(state.messages);\r\n\r\n  // We return a list, because this will get added to the existing list\r\n  // \uae30\uc874 \ubaa9\ub85d\uc5d0 \ucd94\uac00\ub418\uae30 \ub54c\ubb38\uc5d0 \ubaa9\ub85d\uc744 \ubc18\ud658\r\n  return { messages: [response] };\r\n}\r\n\r\n// Define a new graph\r\nconst workflow = new StateGraph(MessagesAnnotation)\r\n  .addNode("agent", callModel)\r\n  .addEdge("__start__", "agent") // __start__ is a special name for the entrypoint\r\n  .addNode("tools", toolNode)\r\n  .addEdge("tools", "agent")\r\n  .addConditionalEdges("agent", shouldContinue);\r\n\r\n// Finally, we compile it into a LangChain Runnable.\r\nconst app = workflow.compile();\r\n\r\n// Use the agent\r\nconst finalState = await app.invoke({\r\n  messages: [new HumanMessage("what is the weather in sf")],\r\n});\r\nconsole.log(finalState.messages[finalState.messages.length - 1].content);\r\n\r\nconst nextState = await app.invoke({\r\n  // Including the messages from the previous run gives the LLM context.\r\n  // This way it knows we\'re asking about the weather in NY\r\n  messages: [...finalState.messages, new HumanMessage("what about ny")],\r\n});\r\nconsole.log(nextState.messages[nextState.messages.length - 1].content);\n'})})]})}function h(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(g,{...n})}):g(n)}},529087:(n,e,t)=>{t.d(e,{R:()=>r,x:()=>i});var a=t(596363);const o={},s=a.createContext(o);function r(n){const e=a.useContext(s);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function i(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:r(n.components),a.createElement(s.Provider,{value:e},n.children)}}}]);