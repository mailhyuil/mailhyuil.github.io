"use strict";(globalThis.webpackChunkmy_blog=globalThis.webpackChunkmy_blog||[]).push([[245443],{148338:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>i,default:()=>c,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"python/python api LoRA","title":"python api LoRA","description":"install","source":"@site/docs/python/python api LoRA.md","sourceDirName":"python","slug":"/python/python api LoRA","permalink":"/docs/python/python api LoRA","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"ai base NLP KoNLPy (\ucf54\uc5d4\uc5d8\ud30c\uc774)","permalink":"/docs/python/python api KoNLPy"},"next":{"title":"python api fastapi grpc","permalink":"/docs/python/python api fastapi grpc"}}');var a=r(447259),o=r(529087);const s={},i="python api LoRA",d={},l=[{value:"install",id:"install",level:2},{value:"fine tuning",id:"fine-tuning",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",pre:"pre",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"python-api-lora",children:"python api LoRA"})}),"\n",(0,a.jsx)(n.h2,{id:"install",children:"install"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sh",children:"poetry add torch # meta\uc5d0\uc11c \uac1c\ubc1c\ud55c \ub525\ub7ec\ub2dd \ud504\ub808\uc784\uc6cc\ud06c (pytorch)\r\npoetry add bitsandbytes # nvidia\uc5d0\uc11c \uac1c\ubc1c\ud55c 8-bit \ubc0f 4-bit \uc591\uc790\ud654(Quantization)\ub85c \ubaa8\ub378\uc744 \uacbd\ub7c9\ud654\ud560 \ub54c \uc0ac\uc6a9\ud558\ub294 \ub77c\uc774\ube0c\ub7ec\ub9ac\r\npoetry add transformers peft datasets accelerate # hugging face api\ub97c \uc0ac\uc6a9\ud558\uae30 \uc704\ud55c \ub77c\uc774\ube0c\ub7ec\ub9ac\r\n\r\npoetry add torch bitsandbytes transformers peft datasets accelerate\n"})}),"\n",(0,a.jsx)(n.h2,{id:"fine-tuning",children:"fine tuning"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-py",children:'import torch\r\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\r\nfrom datasets import load_dataset\r\nfrom peft import LoraConfig, get_peft_model, TaskType\r\n\r\n# \u2705 1. IMDB \ub370\uc774\ud130\uc14b \ub85c\ub4dc\r\ndataset = load_dataset("imdb")\r\ntokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")\r\n\r\ndef preprocess_function(examples):\r\n    return tokenizer(examples["text"], padding="max_length", truncation=True)\r\n\r\ntokenized_datasets = dataset.map(preprocess_function, batched=True)\r\ntokenized_datasets = tokenized_datasets.remove_columns(["text"])\r\ntokenized_datasets.set_format("torch")\r\n\r\ntrain_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(2000))  # \ub370\uc774\ud130 \uc77c\ubd80\ub9cc \uc0ac\uc6a9 (\ube60\ub978 \ud14c\uc2a4\ud2b8)\r\ntest_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(500))\r\n\r\n# \u2705 2. \uc0ac\uc804 \ud6c8\ub828\ub41c \ubaa8\ub378 \ub85c\ub4dc (BERT)\r\nbase_model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)\r\n\r\n# \u2705 3. LoRA \uc124\uc815 (\uc801\uc6a9\ud560 \ub808\uc774\uc5b4 \ubc0f \ub7ad\ud06c \uc124\uc815)\r\nlora_config = LoraConfig(\r\n    task_type=TaskType.SEQ_CLS,  # \uc2dc\ud000\uc2a4 \ubd84\ub958\r\n    inference_mode=False,\r\n    r=8,  # Low-rank \ucc28\uc6d0 (\uac12\uc774 \uc791\uc744\uc218\ub85d \uc801\uc740 \ud30c\ub77c\ubbf8\ud130 \uc5c5\ub370\uc774\ud2b8)\r\n    lora_alpha=16,  # \uc2a4\ucf00\uc77c\ub9c1 \uacc4\uc218\r\n    lora_dropout=0.1,  # \ub4dc\ub86d\uc544\uc6c3 \uc801\uc6a9\r\n    target_modules=["query", "value"],  # LoRA \uc801\uc6a9\ud560 Transformer \ubaa8\ub4c8 (Q, V)\r\n)\r\n\r\n# \u2705 4. LoRA \uc801\uc6a9\r\nlora_model = get_peft_model(base_model, lora_config)\r\nlora_model.print_trainable_parameters()  # \ud6c8\ub828 \uac00\ub2a5\ud55c \ud30c\ub77c\ubbf8\ud130 \uac1c\uc218 \ud655\uc778\r\n\r\n# \u2705 5. \ud6c8\ub828 \uc124\uc815\r\ntraining_args = TrainingArguments(\r\n    output_dir="./lora_imdb",  # \uc800\uc7a5\ud560 \ub514\ub809\ud1a0\ub9ac\r\n    evaluation_strategy="epoch",\r\n    save_strategy="epoch",\r\n    per_device_train_batch_size=8,\r\n    per_device_eval_batch_size=8,\r\n    num_train_epochs=3,\r\n    logging_dir="./logs",\r\n    logging_steps=50,\r\n    save_total_limit=1,\r\n    load_best_model_at_end=True,\r\n    fp16=True,  # GPU \uac00\uc18d\r\n    report_to="none"\r\n)\r\n\r\ntrainer = Trainer(\r\n    model=lora_model,\r\n    args=training_args,\r\n    train_dataset=train_dataset,\r\n    eval_dataset=test_dataset,\r\n    tokenizer=tokenizer,\r\n)\r\n\r\n# \u2705 6. \ubaa8\ub378 \ud6c8\ub828 (LoRA \uc801\uc6a9\ub41c \ubaa8\ub378 \ud559\uc2b5)\r\ntrainer.train()\r\n\r\n# \u2705 7. \ud3c9\uac00\r\ntrainer.evaluate()\r\n\r\n# \u2705 8. LoRA \ubaa8\ub378 & \ud1a0\ud06c\ub098\uc774\uc800 \uc800\uc7a5\r\nsave_directory = "./lora_imdb_saved"\r\n\r\ntrainer.save_model(save_directory)  # \ubaa8\ub378 \uc800\uc7a5\r\ntokenizer.save_pretrained(save_directory)  # \ud1a0\ud06c\ub098\uc774\uc800 \uc800\uc7a5\r\n\r\nprint(f"LoRA \ubaa8\ub378\uc774 \'{save_directory}\'\uc5d0 \uc800\uc7a5\ub418\uc5c8\uc2b5\ub2c8\ub2e4.")\n'})})]})}function c(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},529087:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>i});var t=r(596363);const a={},o=t.createContext(a);function s(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);