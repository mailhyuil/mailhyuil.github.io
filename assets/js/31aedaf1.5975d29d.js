"use strict";(self.webpackChunkmy_blog=self.webpackChunkmy_blog||[]).push([["600800"],{837436:function(n,e,t){t.r(e),t.d(e,{frontMatter:()=>r,toc:()=>c,default:()=>h,metadata:()=>a,assets:()=>l,contentTitle:()=>i});var a=JSON.parse('{"id":"ai langchain/langchain LangGraph custom agent","title":"langchain LangGraph custom agent","description":"","source":"@site/docs/ai langchain/langchain LangGraph custom agent.md","sourceDirName":"ai langchain","slug":"/ai langchain/langchain LangGraph custom agent","permalink":"/docs/ai langchain/langchain LangGraph custom agent","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"langchain LangGraph createReactAgent","permalink":"/docs/ai langchain/langchain LangGraph createReactAgent"},"next":{"title":"langchain LangGraph graph (workflow)","permalink":"/docs/ai langchain/langchain LangGraph graph - workflow"}}'),o=t(447259),s=t(255511);let r={},i="langchain LangGraph custom agent",l={},c=[];function g(n){let e={code:"code",h1:"h1",header:"header",pre:"pre",...(0,s.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"langchain-langgraph-custom-agent",children:"langchain LangGraph custom agent"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-ts",children:'import { TavilySearchResults } from "@langchain/community/tools/tavily_search";\r\nimport { ChatOpenAI } from "@langchain/openai";\r\nimport { HumanMessage, AIMessage } from "@langchain/core/messages";\r\nimport { ToolNode } from "@langchain/langgraph/prebuilt";\r\nimport { StateGraph, MessagesAnnotation } from "@langchain/langgraph";\r\n\r\n// Define the tools for the agent to use\r\nconst tools = [new TavilySearchResults({ maxResults: 3 })];\r\nconst toolNode = new ToolNode(tools);\r\n\r\n// Create a model and give it access to the tools\r\nconst model = new ChatOpenAI({\r\n  model: "gpt-4o-mini",\r\n  temperature: 0,\r\n}).bindTools(tools);\r\n\r\n// Define the function that determines whether to continue or not\r\n// \uACC4\uC18D\uD560\uC9C0 \uB9D0\uC9C0 \uACB0\uC815\uD558\uB294 \uD568\uC218 \uC815\uC758\r\nfunction shouldContinue({ messages }: typeof MessagesAnnotation.State) {\r\n  const lastMessage = messages[messages.length - 1] as AIMessage;\r\n\r\n  // If the LLM makes a tool call, then we route to the "tools" node\r\n  // \uB3C4\uAD6C \uD638\uCD9C\uC774 \uC788\uC73C\uBA74 "tools" \uB178\uB4DC\uB85C \uB77C\uC6B0\uD305\r\n  if (lastMessage.tool_calls?.length) {\r\n    return "tools";\r\n  }\r\n  // Otherwise, we stop (reply to the user) using the special "__end__" node\r\n  // \uADF8\uB807\uC9C0 \uC54A\uC73C\uBA74 \uD2B9\uC218\uD55C "__end__" \uB178\uB4DC\uB97C \uC0AC\uC6A9\uD558\uC5EC \uBA48\uCDA4\r\n  return "__end__";\r\n}\r\n\r\n// Define the function that calls the model\r\n// \uBAA8\uB378\uC744 \uD638\uCD9C\uD558\uB294 \uD568\uC218 \uC815\uC758\r\nasync function callModel(state: typeof MessagesAnnotation.State) {\r\n  const response = await model.invoke(state.messages);\r\n\r\n  // We return a list, because this will get added to the existing list\r\n  // \uAE30\uC874 \uBAA9\uB85D\uC5D0 \uCD94\uAC00\uB418\uAE30 \uB54C\uBB38\uC5D0 \uBAA9\uB85D\uC744 \uBC18\uD658\r\n  return { messages: [response] };\r\n}\r\n\r\n// Define a new graph\r\nconst workflow = new StateGraph(MessagesAnnotation)\r\n  .addNode("agent", callModel)\r\n  .addEdge("__start__", "agent") // __start__ is a special name for the entrypoint\r\n  .addNode("tools", toolNode)\r\n  .addEdge("tools", "agent")\r\n  .addConditionalEdges("agent", shouldContinue);\r\n\r\n// Finally, we compile it into a LangChain Runnable.\r\nconst app = workflow.compile();\r\n\r\n// Use the agent\r\nconst finalState = await app.invoke({\r\n  messages: [new HumanMessage("what is the weather in sf")],\r\n});\r\nconsole.log(finalState.messages[finalState.messages.length - 1].content);\r\n\r\nconst nextState = await app.invoke({\r\n  // Including the messages from the previous run gives the LLM context.\r\n  // This way it knows we\'re asking about the weather in NY\r\n  messages: [...finalState.messages, new HumanMessage("what about ny")],\r\n});\r\nconsole.log(nextState.messages[nextState.messages.length - 1].content);\n'})})]})}function h(n={}){let{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(g,{...n})}):g(n)}},255511:function(n,e,t){t.d(e,{R:()=>r,x:()=>i});var a=t(596363);let o={},s=a.createContext(o);function r(n){let e=a.useContext(s);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function i(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:r(n.components),a.createElement(s.Provider,{value:e},n.children)}}}]);