"use strict";(globalThis.webpackChunkmy_blog=globalThis.webpackChunkmy_blog||[]).push([[817241],{529087:(n,e,a)=>{a.d(e,{R:()=>o,x:()=>l});var t=a(596363);const r={},s=t.createContext(r);function o(n){const e=t.useContext(s);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:o(n.components),t.createElement(s.Provider,{value:e},n.children)}},759102:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>i,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"ai langchain/langchain LangGraph","title":"langchain LangGraph","description":"LangChain\uc744 \uae30\ubc18\uc73c\ub85c \ud55c AI \uc6cc\ud06c\ud50c\ub85c\uc6b0 \uc624\ucf00\uc2a4\ud2b8\ub808\uc774\uc158 \ub3c4\uad6c\uc785\ub2c8\ub2e4.","source":"@site/docs/ai langchain/langchain LangGraph.md","sourceDirName":"ai langchain","slug":"/ai langchain/langchain LangGraph","permalink":"/docs/ai langchain/langchain LangGraph","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"langchain LangGraph graph (workflow)","permalink":"/docs/ai langchain/langchain LangGraph graph - workflow"},"next":{"title":"langchain LangSmith","permalink":"/docs/ai langchain/langchain LangSmith"}}');var r=a(447259),s=a(529087);const o={},l="langchain LangGraph",i={},c=[{value:"install",id:"install",level:2},{value:"createReactAgent",id:"createreactagent",level:2},{value:"custom agent",id:"custom-agent",level:2}];function g(n){const e={blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",...(0,s.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"langchain-langgraph",children:"langchain LangGraph"})}),"\n",(0,r.jsxs)(e.blockquote,{children:["\n",(0,r.jsx)(e.p,{children:"LangChain\uc744 \uae30\ubc18\uc73c\ub85c \ud55c AI \uc6cc\ud06c\ud50c\ub85c\uc6b0 \uc624\ucf00\uc2a4\ud2b8\ub808\uc774\uc158 \ub3c4\uad6c\uc785\ub2c8\ub2e4."}),"\n",(0,r.jsxs)(e.blockquote,{children:["\n",(0,r.jsx)(e.p,{children:"\uc989, \uba40\ud2f0-\uc5d0\uc774\uc804\ud2b8 \uc2dc\uc2a4\ud15c\uc774\ub098 \ubcf5\uc7a1\ud55c AI \ud30c\uc774\ud504\ub77c\uc778\uc744 \uad6c\ucd95\ud560 \ub54c \uc0ac\uc6a9\ub429\ub2c8\ub2e4."}),"\n",(0,r.jsxs)(e.blockquote,{children:["\n",(0,r.jsx)(e.p,{children:"LangChain\uc740 \uc77c\ub82c\ub85c \ub85c\uc9c1\uc744 \uc2e4\ud589(\uccb4\uc774\ub2dd)\ud558\ub294 \ubc29\uc2dd\uc774\uc9c0\ub9cc"}),"\n",(0,r.jsx)(e.p,{children:"LangGraph\ub294 \uadf8\ub798\ud504\uad6c\uc870/\uc0c1\ud0dc\uad00\ub9ac/\ubcd1\ub82c\uc2e4\ud589/\uc870\uac74\uc2e4\ud589 \ub4f1\uc758 \ud2b9\uc9d5\uc73c\ub85c \ub354 \ubcf5\uc7a1\ud55c \uc2dc\uc2a4\ud15c\uc744 \uad6c\ucd95\ud560 \uc218 \uc788\ub2e4."}),"\n",(0,r.jsxs)(e.blockquote,{children:["\n",(0,r.jsx)(e.p,{children:"Graph\ub780 AI\uac00 \ub3d9\uc791\ud574\uc57c\ud558\ub294 Workflow\ub97c \uc758\ubbf8\ud55c\ub2e4."}),"\n",(0,r.jsxs)(e.blockquote,{children:["\n",(0,r.jsx)(e.p,{children:"\uc0ac\uc6a9\uc790\uac00 \uc5ec\ub7ec\ubc88 \uc9c8\ubb38\uc744 \ud560 \ub54c\ub9c8\ub2e4 \uc774\uc804 \uc0c1\ud0dc\ub97c \uae30\uc5b5\ud558\uace0\uc788\ub2e4\uac00 \ub2e4\uc74c \ub178\ub4dc\ub85c \uc774\ub3d9\uc2dc\ud0a8\ub2e4."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"install",children:"install"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-sh",children:"npm i @langchain/core\r\nnpm i @langchain/langgraph\r\n\r\nnpm i @langchain/openai\n"})}),"\n",(0,r.jsx)(e.h2,{id:"createreactagent",children:"createReactAgent"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-ts",children:'// agent.ts\r\nimport { TavilySearchResults } from "@langchain/community/tools/tavily_search";\r\nimport { ChatOpenAI } from "@langchain/openai";\r\nimport { MemorySaver } from "@langchain/langgraph";\r\nimport { HumanMessage } from "@langchain/core/messages";\r\nimport { createReactAgent } from "@langchain/langgraph/prebuilt";\r\n\r\n// Define the tools for the agent to use\r\nconst agentTools = [new TavilySearchResults({ maxResults: 3 })];\r\nconst agentModel = new ChatOpenAI({ temperature: 0 });\r\n\r\n// Initialize memory to persist state between graph runs\r\nconst agentCheckpointer = new MemorySaver();\r\nconst agent = createReactAgent({\r\n  llm: agentModel,\r\n  tools: agentTools,\r\n  checkpointSaver: agentCheckpointer,\r\n});\r\n\r\n// Now it\'s time to use!\r\nconst agentFinalState = await agent.invoke(\r\n  { messages: [new HumanMessage("what is the current weather in sf")] },\r\n  { configurable: { thread_id: "42" } },\r\n);\r\n\r\nconsole.log(agentFinalState.messages[agentFinalState.messages.length - 1].content);\r\n\r\nconst agentNextState = await agent.invoke(\r\n  { messages: [new HumanMessage("what about ny")] },\r\n  { configurable: { thread_id: "42" } },\r\n);\r\n\r\nconsole.log(agentNextState.messages[agentNextState.messages.length - 1].content);\n'})}),"\n",(0,r.jsx)(e.h2,{id:"custom-agent",children:"custom agent"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-ts",children:'import { TavilySearchResults } from "@langchain/community/tools/tavily_search";\r\nimport { ChatOpenAI } from "@langchain/openai";\r\nimport { HumanMessage, AIMessage } from "@langchain/core/messages";\r\nimport { ToolNode } from "@langchain/langgraph/prebuilt";\r\nimport { StateGraph, MessagesAnnotation } from "@langchain/langgraph";\r\n\r\n// Define the tools for the agent to use\r\nconst tools = [new TavilySearchResults({ maxResults: 3 })];\r\nconst toolNode = new ToolNode(tools);\r\n\r\n// Create a model and give it access to the tools\r\nconst model = new ChatOpenAI({\r\n  model: "gpt-4o-mini",\r\n  temperature: 0,\r\n}).bindTools(tools);\r\n\r\n// Define the function that determines whether to continue or not\r\n// \uacc4\uc18d\ud560\uc9c0 \ub9d0\uc9c0 \uacb0\uc815\ud558\ub294 \ud568\uc218 \uc815\uc758\r\nfunction shouldContinue({ messages }: typeof MessagesAnnotation.State) {\r\n  const lastMessage = messages[messages.length - 1] as AIMessage;\r\n\r\n  // If the LLM makes a tool call, then we route to the "tools" node\r\n  // \ub3c4\uad6c \ud638\ucd9c\uc774 \uc788\uc73c\uba74 "tools" \ub178\ub4dc\ub85c \ub77c\uc6b0\ud305\r\n  if (lastMessage.tool_calls?.length) {\r\n    return "tools";\r\n  }\r\n  // Otherwise, we stop (reply to the user) using the special "__end__" node\r\n  // \uadf8\ub807\uc9c0 \uc54a\uc73c\uba74 \ud2b9\uc218\ud55c "__end__" \ub178\ub4dc\ub97c \uc0ac\uc6a9\ud558\uc5ec \uba48\ucda4\r\n  return "__end__";\r\n}\r\n\r\n// Define the function that calls the model\r\n// \ubaa8\ub378\uc744 \ud638\ucd9c\ud558\ub294 \ud568\uc218 \uc815\uc758\r\nasync function callModel(state: typeof MessagesAnnotation.State) {\r\n  const response = await model.invoke(state.messages);\r\n\r\n  // We return a list, because this will get added to the existing list\r\n  // \uae30\uc874 \ubaa9\ub85d\uc5d0 \ucd94\uac00\ub418\uae30 \ub54c\ubb38\uc5d0 \ubaa9\ub85d\uc744 \ubc18\ud658\r\n  return { messages: [response] };\r\n}\r\n\r\n// Define a new graph\r\nconst workflow = new StateGraph(MessagesAnnotation)\r\n  .addNode("agent", callModel)\r\n  .addEdge("__start__", "agent") // __start__ is a special name for the entrypoint\r\n  .addNode("tools", toolNode)\r\n  .addEdge("tools", "agent")\r\n  .addConditionalEdges("agent", shouldContinue);\r\n\r\n// Finally, we compile it into a LangChain Runnable.\r\nconst app = workflow.compile();\r\n\r\n// Use the agent\r\nconst finalState = await app.invoke({\r\n  messages: [new HumanMessage("what is the weather in sf")],\r\n});\r\nconsole.log(finalState.messages[finalState.messages.length - 1].content);\r\n\r\nconst nextState = await app.invoke({\r\n  // Including the messages from the previous run gives the LLM context.\r\n  // This way it knows we\'re asking about the weather in NY\r\n  messages: [...finalState.messages, new HumanMessage("what about ny")],\r\n});\r\nconsole.log(nextState.messages[nextState.messages.length - 1].content);\n'})})]})}function h(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(g,{...n})}):g(n)}}}]);