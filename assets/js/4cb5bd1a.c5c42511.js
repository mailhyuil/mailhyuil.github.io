"use strict";(self.webpackChunkmy_blog=self.webpackChunkmy_blog||[]).push([["811776"],{707607:function(e,n,r){r.r(n),r.d(n,{frontMatter:()=>i,toc:()=>l,default:()=>c,metadata:()=>t,assets:()=>d,contentTitle:()=>s});var t=JSON.parse('{"id":"python/python api LoRA","title":"python api LoRA","description":"install","source":"@site/docs/python/python api LoRA.md","sourceDirName":"python","slug":"/python/python api LoRA","permalink":"/docs/python/python api LoRA","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"ai base NLP KoNLPy (\uCF54\uC5D4\uC5D8\uD30C\uC774)","permalink":"/docs/python/python api KoNLPy"},"next":{"title":"python api fastapi grpc","permalink":"/docs/python/python api fastapi grpc"}}'),a=r(447259),o=r(255511);let i={},s="python api LoRA",d={},l=[{value:"install",id:"install",level:2},{value:"fine tuning",id:"fine-tuning",level:2}];function p(e){let n={code:"code",h1:"h1",h2:"h2",header:"header",pre:"pre",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"python-api-lora",children:"python api LoRA"})}),"\n",(0,a.jsx)(n.h2,{id:"install",children:"install"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sh",children:"poetry add torch # meta\uC5D0\uC11C \uAC1C\uBC1C\uD55C \uB525\uB7EC\uB2DD \uD504\uB808\uC784\uC6CC\uD06C (pytorch)\r\npoetry add bitsandbytes # nvidia\uC5D0\uC11C \uAC1C\uBC1C\uD55C 8-bit \uBC0F 4-bit \uC591\uC790\uD654(Quantization)\uB85C \uBAA8\uB378\uC744 \uACBD\uB7C9\uD654\uD560 \uB54C \uC0AC\uC6A9\uD558\uB294 \uB77C\uC774\uBE0C\uB7EC\uB9AC\r\npoetry add transformers peft datasets accelerate # hugging face api\uB97C \uC0AC\uC6A9\uD558\uAE30 \uC704\uD55C \uB77C\uC774\uBE0C\uB7EC\uB9AC\r\n\r\npoetry add torch bitsandbytes transformers peft datasets accelerate\n"})}),"\n",(0,a.jsx)(n.h2,{id:"fine-tuning",children:"fine tuning"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-py",children:'import torch\r\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\r\nfrom datasets import load_dataset\r\nfrom peft import LoraConfig, get_peft_model, TaskType\r\n\r\n# \u2705 1. IMDB \uB370\uC774\uD130\uC14B \uB85C\uB4DC\r\ndataset = load_dataset("imdb")\r\ntokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")\r\n\r\ndef preprocess_function(examples):\r\n    return tokenizer(examples["text"], padding="max_length", truncation=True)\r\n\r\ntokenized_datasets = dataset.map(preprocess_function, batched=True)\r\ntokenized_datasets = tokenized_datasets.remove_columns(["text"])\r\ntokenized_datasets.set_format("torch")\r\n\r\ntrain_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(2000))  # \uB370\uC774\uD130 \uC77C\uBD80\uB9CC \uC0AC\uC6A9 (\uBE60\uB978 \uD14C\uC2A4\uD2B8)\r\ntest_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(500))\r\n\r\n# \u2705 2. \uC0AC\uC804 \uD6C8\uB828\uB41C \uBAA8\uB378 \uB85C\uB4DC (BERT)\r\nbase_model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)\r\n\r\n# \u2705 3. LoRA \uC124\uC815 (\uC801\uC6A9\uD560 \uB808\uC774\uC5B4 \uBC0F \uB7AD\uD06C \uC124\uC815)\r\nlora_config = LoraConfig(\r\n    task_type=TaskType.SEQ_CLS,  # \uC2DC\uD000\uC2A4 \uBD84\uB958\r\n    inference_mode=False,\r\n    r=8,  # Low-rank \uCC28\uC6D0 (\uAC12\uC774 \uC791\uC744\uC218\uB85D \uC801\uC740 \uD30C\uB77C\uBBF8\uD130 \uC5C5\uB370\uC774\uD2B8)\r\n    lora_alpha=16,  # \uC2A4\uCF00\uC77C\uB9C1 \uACC4\uC218\r\n    lora_dropout=0.1,  # \uB4DC\uB86D\uC544\uC6C3 \uC801\uC6A9\r\n    target_modules=["query", "value"],  # LoRA \uC801\uC6A9\uD560 Transformer \uBAA8\uB4C8 (Q, V)\r\n)\r\n\r\n# \u2705 4. LoRA \uC801\uC6A9\r\nlora_model = get_peft_model(base_model, lora_config)\r\nlora_model.print_trainable_parameters()  # \uD6C8\uB828 \uAC00\uB2A5\uD55C \uD30C\uB77C\uBBF8\uD130 \uAC1C\uC218 \uD655\uC778\r\n\r\n# \u2705 5. \uD6C8\uB828 \uC124\uC815\r\ntraining_args = TrainingArguments(\r\n    output_dir="./lora_imdb",  # \uC800\uC7A5\uD560 \uB514\uB809\uD1A0\uB9AC\r\n    evaluation_strategy="epoch",\r\n    save_strategy="epoch",\r\n    per_device_train_batch_size=8,\r\n    per_device_eval_batch_size=8,\r\n    num_train_epochs=3,\r\n    logging_dir="./logs",\r\n    logging_steps=50,\r\n    save_total_limit=1,\r\n    load_best_model_at_end=True,\r\n    fp16=True,  # GPU \uAC00\uC18D\r\n    report_to="none"\r\n)\r\n\r\ntrainer = Trainer(\r\n    model=lora_model,\r\n    args=training_args,\r\n    train_dataset=train_dataset,\r\n    eval_dataset=test_dataset,\r\n    tokenizer=tokenizer,\r\n)\r\n\r\n# \u2705 6. \uBAA8\uB378 \uD6C8\uB828 (LoRA \uC801\uC6A9\uB41C \uBAA8\uB378 \uD559\uC2B5)\r\ntrainer.train()\r\n\r\n# \u2705 7. \uD3C9\uAC00\r\ntrainer.evaluate()\r\n\r\n# \u2705 8. LoRA \uBAA8\uB378 & \uD1A0\uD06C\uB098\uC774\uC800 \uC800\uC7A5\r\nsave_directory = "./lora_imdb_saved"\r\n\r\ntrainer.save_model(save_directory)  # \uBAA8\uB378 \uC800\uC7A5\r\ntokenizer.save_pretrained(save_directory)  # \uD1A0\uD06C\uB098\uC774\uC800 \uC800\uC7A5\r\n\r\nprint(f"LoRA \uBAA8\uB378\uC774 \'{save_directory}\'\uC5D0 \uC800\uC7A5\uB418\uC5C8\uC2B5\uB2C8\uB2E4.")\n'})})]})}function c(e={}){let{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},255511:function(e,n,r){r.d(n,{R:()=>i,x:()=>s});var t=r(596363);let a={},o=t.createContext(a);function i(e){let n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);