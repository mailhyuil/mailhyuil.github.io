"use strict";(self.webpackChunkmy_blog=self.webpackChunkmy_blog||[]).push([["879298"],{616272:function(n,e,a){a.r(e),a.d(e,{frontMatter:()=>o,toc:()=>c,default:()=>h,metadata:()=>t,assets:()=>i,contentTitle:()=>l});var t=JSON.parse('{"id":"ai langchain/langchain LangGraph","title":"langchain LangGraph","description":"LangChain\uC744 \uAE30\uBC18\uC73C\uB85C \uD55C AI \uC6CC\uD06C\uD50C\uB85C\uC6B0 \uC624\uCF00\uC2A4\uD2B8\uB808\uC774\uC158 \uB3C4\uAD6C\uC785\uB2C8\uB2E4.","source":"@site/docs/ai langchain/langchain LangGraph.md","sourceDirName":"ai langchain","slug":"/ai langchain/langchain LangGraph","permalink":"/docs/ai langchain/langchain LangGraph","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"langchain LangGraph graph (workflow)","permalink":"/docs/ai langchain/langchain LangGraph graph - workflow"},"next":{"title":"langchain LangSmith","permalink":"/docs/ai langchain/langchain LangSmith"}}'),r=a(447259),s=a(255511);let o={},l="langchain LangGraph",i={},c=[{value:"install",id:"install",level:2},{value:"createReactAgent",id:"createreactagent",level:2},{value:"custom agent",id:"custom-agent",level:2}];function g(n){let e={blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",...(0,s.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"langchain-langgraph",children:"langchain LangGraph"})}),"\n",(0,r.jsxs)(e.blockquote,{children:["\n",(0,r.jsx)(e.p,{children:"LangChain\uC744 \uAE30\uBC18\uC73C\uB85C \uD55C AI \uC6CC\uD06C\uD50C\uB85C\uC6B0 \uC624\uCF00\uC2A4\uD2B8\uB808\uC774\uC158 \uB3C4\uAD6C\uC785\uB2C8\uB2E4."}),"\n",(0,r.jsxs)(e.blockquote,{children:["\n",(0,r.jsx)(e.p,{children:"\uC989, \uBA40\uD2F0-\uC5D0\uC774\uC804\uD2B8 \uC2DC\uC2A4\uD15C\uC774\uB098 \uBCF5\uC7A1\uD55C AI \uD30C\uC774\uD504\uB77C\uC778\uC744 \uAD6C\uCD95\uD560 \uB54C \uC0AC\uC6A9\uB429\uB2C8\uB2E4."}),"\n",(0,r.jsxs)(e.blockquote,{children:["\n",(0,r.jsx)(e.p,{children:"LangChain\uC740 \uC77C\uB82C\uB85C \uB85C\uC9C1\uC744 \uC2E4\uD589(\uCCB4\uC774\uB2DD)\uD558\uB294 \uBC29\uC2DD\uC774\uC9C0\uB9CC"}),"\n",(0,r.jsx)(e.p,{children:"LangGraph\uB294 \uADF8\uB798\uD504\uAD6C\uC870/\uC0C1\uD0DC\uAD00\uB9AC/\uBCD1\uB82C\uC2E4\uD589/\uC870\uAC74\uC2E4\uD589 \uB4F1\uC758 \uD2B9\uC9D5\uC73C\uB85C \uB354 \uBCF5\uC7A1\uD55C \uC2DC\uC2A4\uD15C\uC744 \uAD6C\uCD95\uD560 \uC218 \uC788\uB2E4."}),"\n",(0,r.jsxs)(e.blockquote,{children:["\n",(0,r.jsx)(e.p,{children:"Graph\uB780 AI\uAC00 \uB3D9\uC791\uD574\uC57C\uD558\uB294 Workflow\uB97C \uC758\uBBF8\uD55C\uB2E4."}),"\n",(0,r.jsxs)(e.blockquote,{children:["\n",(0,r.jsx)(e.p,{children:"\uC0AC\uC6A9\uC790\uAC00 \uC5EC\uB7EC\uBC88 \uC9C8\uBB38\uC744 \uD560 \uB54C\uB9C8\uB2E4 \uC774\uC804 \uC0C1\uD0DC\uB97C \uAE30\uC5B5\uD558\uACE0\uC788\uB2E4\uAC00 \uB2E4\uC74C \uB178\uB4DC\uB85C \uC774\uB3D9\uC2DC\uD0A8\uB2E4."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"install",children:"install"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-sh",children:"npm i @langchain/core\r\nnpm i @langchain/langgraph\r\n\r\nnpm i @langchain/openai\n"})}),"\n",(0,r.jsx)(e.h2,{id:"createreactagent",children:"createReactAgent"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-ts",children:'// agent.ts\r\nimport { TavilySearchResults } from "@langchain/community/tools/tavily_search";\r\nimport { ChatOpenAI } from "@langchain/openai";\r\nimport { MemorySaver } from "@langchain/langgraph";\r\nimport { HumanMessage } from "@langchain/core/messages";\r\nimport { createReactAgent } from "@langchain/langgraph/prebuilt";\r\n\r\n// Define the tools for the agent to use\r\nconst agentTools = [new TavilySearchResults({ maxResults: 3 })];\r\nconst agentModel = new ChatOpenAI({ temperature: 0 });\r\n\r\n// Initialize memory to persist state between graph runs\r\nconst agentCheckpointer = new MemorySaver();\r\nconst agent = createReactAgent({\r\n  llm: agentModel,\r\n  tools: agentTools,\r\n  checkpointSaver: agentCheckpointer,\r\n});\r\n\r\n// Now it\'s time to use!\r\nconst agentFinalState = await agent.invoke(\r\n  { messages: [new HumanMessage("what is the current weather in sf")] },\r\n  { configurable: { thread_id: "42" } },\r\n);\r\n\r\nconsole.log(agentFinalState.messages[agentFinalState.messages.length - 1].content);\r\n\r\nconst agentNextState = await agent.invoke(\r\n  { messages: [new HumanMessage("what about ny")] },\r\n  { configurable: { thread_id: "42" } },\r\n);\r\n\r\nconsole.log(agentNextState.messages[agentNextState.messages.length - 1].content);\n'})}),"\n",(0,r.jsx)(e.h2,{id:"custom-agent",children:"custom agent"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-ts",children:'import { TavilySearchResults } from "@langchain/community/tools/tavily_search";\r\nimport { ChatOpenAI } from "@langchain/openai";\r\nimport { HumanMessage, AIMessage } from "@langchain/core/messages";\r\nimport { ToolNode } from "@langchain/langgraph/prebuilt";\r\nimport { StateGraph, MessagesAnnotation } from "@langchain/langgraph";\r\n\r\n// Define the tools for the agent to use\r\nconst tools = [new TavilySearchResults({ maxResults: 3 })];\r\nconst toolNode = new ToolNode(tools);\r\n\r\n// Create a model and give it access to the tools\r\nconst model = new ChatOpenAI({\r\n  model: "gpt-4o-mini",\r\n  temperature: 0,\r\n}).bindTools(tools);\r\n\r\n// Define the function that determines whether to continue or not\r\n// \uACC4\uC18D\uD560\uC9C0 \uB9D0\uC9C0 \uACB0\uC815\uD558\uB294 \uD568\uC218 \uC815\uC758\r\nfunction shouldContinue({ messages }: typeof MessagesAnnotation.State) {\r\n  const lastMessage = messages[messages.length - 1] as AIMessage;\r\n\r\n  // If the LLM makes a tool call, then we route to the "tools" node\r\n  // \uB3C4\uAD6C \uD638\uCD9C\uC774 \uC788\uC73C\uBA74 "tools" \uB178\uB4DC\uB85C \uB77C\uC6B0\uD305\r\n  if (lastMessage.tool_calls?.length) {\r\n    return "tools";\r\n  }\r\n  // Otherwise, we stop (reply to the user) using the special "__end__" node\r\n  // \uADF8\uB807\uC9C0 \uC54A\uC73C\uBA74 \uD2B9\uC218\uD55C "__end__" \uB178\uB4DC\uB97C \uC0AC\uC6A9\uD558\uC5EC \uBA48\uCDA4\r\n  return "__end__";\r\n}\r\n\r\n// Define the function that calls the model\r\n// \uBAA8\uB378\uC744 \uD638\uCD9C\uD558\uB294 \uD568\uC218 \uC815\uC758\r\nasync function callModel(state: typeof MessagesAnnotation.State) {\r\n  const response = await model.invoke(state.messages);\r\n\r\n  // We return a list, because this will get added to the existing list\r\n  // \uAE30\uC874 \uBAA9\uB85D\uC5D0 \uCD94\uAC00\uB418\uAE30 \uB54C\uBB38\uC5D0 \uBAA9\uB85D\uC744 \uBC18\uD658\r\n  return { messages: [response] };\r\n}\r\n\r\n// Define a new graph\r\nconst workflow = new StateGraph(MessagesAnnotation)\r\n  .addNode("agent", callModel)\r\n  .addEdge("__start__", "agent") // __start__ is a special name for the entrypoint\r\n  .addNode("tools", toolNode)\r\n  .addEdge("tools", "agent")\r\n  .addConditionalEdges("agent", shouldContinue);\r\n\r\n// Finally, we compile it into a LangChain Runnable.\r\nconst app = workflow.compile();\r\n\r\n// Use the agent\r\nconst finalState = await app.invoke({\r\n  messages: [new HumanMessage("what is the weather in sf")],\r\n});\r\nconsole.log(finalState.messages[finalState.messages.length - 1].content);\r\n\r\nconst nextState = await app.invoke({\r\n  // Including the messages from the previous run gives the LLM context.\r\n  // This way it knows we\'re asking about the weather in NY\r\n  messages: [...finalState.messages, new HumanMessage("what about ny")],\r\n});\r\nconsole.log(nextState.messages[nextState.messages.length - 1].content);\n'})})]})}function h(n={}){let{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(g,{...n})}):g(n)}},255511:function(n,e,a){a.d(e,{R:()=>o,x:()=>l});var t=a(596363);let r={},s=t.createContext(r);function o(n){let e=t.useContext(s);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:o(n.components),t.createElement(s.Provider,{value:e},n.children)}}}]);